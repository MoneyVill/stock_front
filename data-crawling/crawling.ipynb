{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pymysql\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib\n",
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 환율"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### api 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def exchange_rate_api(date):\n",
    "#   API_HOST = \"https://www.koreaexim.go.kr/site/program/financial/exchangeJSON\"\n",
    "#   key = \"?authkey=td5BrimjfAfpsjRDGqdnAPoo1yxZ6ngI\"\n",
    "#   search = \"&searchdate=\"+date\n",
    "#   data = \"&data=AP01\"\n",
    "  \n",
    "#   url = API_HOST+key+search+data\n",
    "#   headers = {\"Content-Type\" : \"application/json\", \"charset\" : \"UTF-8\", \"Accept\":\"*/*\"}\n",
    "\n",
    "#   try:\n",
    "#     response = requests.get(url, headers=headers)\n",
    "#     return response\n",
    "#   except Exception as ex:\n",
    "#     print(ex)\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_range = pd.date_range(start='1/1/2024', end='10/31/2024')\n",
    "\n",
    "# result_list = []\n",
    "# for date in tqdm(date_range):\n",
    "#     if dt.date.weekday(date) in (5, 6):\n",
    "#         continue\n",
    "#     str_date = str(date).split(\" \")[0]\n",
    "    \n",
    "#     response = exchange_rate_api(str_date)\n",
    "#     if not response:  # 요청 실패 시 다음 날짜로 넘어감\n",
    "#         continue\n",
    "\n",
    "#     try:\n",
    "#         r = json.loads(response.text)\n",
    "#     except json.JSONDecodeError:\n",
    "#         print(f\"Error decoding JSON for date {str_date}\")\n",
    "#         continue\n",
    "\n",
    "#     flag = True\n",
    "#     for data in r:\n",
    "#         if data[\"result\"] == 4:\n",
    "#             flag = False\n",
    "#             break\n",
    "#         if data[\"cur_nm\"] in ('미국 달러', '일본 옌', '유로'):\n",
    "#             if data[\"cur_nm\"] == '일본 옌':\n",
    "#                 result_list.append(\"('{}', '{}', {})\".format(str_date, '일본 100엔', data[\"deal_bas_r\"].replace(\",\", \"\")))\n",
    "#             else:\n",
    "#                 result_list.append(\"('{}', '{}', {})\".format(str_date, data['cur_nm'], data[\"deal_bas_r\"].replace(\",\", \"\")))\n",
    "#     if not flag:\n",
    "#         break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 저장 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data(cur_20241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data(cur_20241126).p\", \"rb\") as f:\n",
    "#   result_list = pickle.load(f)\n",
    "# result_list[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DB 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()"
    "# con = pymysql.connect(host='localhost', user='ssafy', password='ssafy', \n",
    "#                       db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "# cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql = \"\"\"CREATE TABLE IF NOT EXISTS currency (\n",
    "#     cur_date DATE NOT NULL,\n",
    "#     cur_name VARCHAR(50) NOT NULL,\n",
    "#     cur_rate FLOAT NOT NULL\n",
    "# ) CHARSET=utf8mb4;\"\"\"\n",
    "# cur.execute(sql)\n",
    "# sql = \"INSERT INTO currency(cur_date, cur_name, cur_rate) VALUES \" + \",\".join(result_list)\n",
    "# cur.execute(sql)\n",
    "# con.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원자재(석유, 금)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- API가 2020년부터 데이터가 있어서 사용이 불가능.. 하하\n",
    "- Naver를 통해 크롤링해야할 듯.\n",
    "  - 주식, 코스닥 코스피도 동일\n",
    "- 석유 : 경유, 휘발유, 등유 3가지 평균값 활용\n",
    "  - 네이버 증권에는 등유가 없어서 경유, 휘발유만 사용?\n",
    "- 금 : 국내금, 국제금 2가지 각각 활용"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### api 함수"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 오일\n",
    "- 휘발유 : OIL_GSL\n",
    "- 경유 : OIL_LO\n",
    "- 예시 : https://finance.naver.com/marketindex/oilDailyQuote.naver?marketindexCd=OIL_GSL&page=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def oil_api(oil_type):\n",
    "#   page = 48\n",
    "#   end = 4\n",
    "#   oil = \"휘발유\" if oil_type == \"OIL_GSL\" else \"경유\"\n",
    "#   HOST = \"https://finance.naver.com/marketindex/oilDailyQuote.naver?marketindexCd={}&page=\".format(oil_type)\n",
    "  \n",
    "#   headers = {\"Content-Type\" : \"application/json\", \"charset\" : \"UTF-8\", \"Accept\":\"*/*\"}\n",
    "\n",
    "#   try:\n",
    "#     result_list = []\n",
    "#     for p in tqdm(range(page, end-1, -1), \n",
    "#               total = page-end+1, ## 전체 진행수\n",
    "#               desc = 'Desc', ## 진행률 앞쪽 출력 문장\n",
    "#               ncols = 80, ## 진행률 출력 폭 조절\n",
    "#               leave = True, ## True 반복문 완료시 진행률 출력 남김. False 남기지 않음.\n",
    "#             ):\n",
    "#       url = HOST+str(p)\n",
    "#       response = requests.get(url, headers=headers)\n",
    "#       result_list.extend(oil_bs(response, oil))\n",
    "      \n",
    "#       time.sleep(0.5)\n",
    "      \n",
    "#     return result_list\n",
    "#   except Exception as ex:\n",
    "#     print(\"oil_api 오류 발생\")\n",
    "#     print(ex)\n",
    "\n",
    "# def oil_bs(response, oil_type):\n",
    "#   start = dt.datetime.strptime(\"2024-01-01\", \"%Y-%m-%d\")\n",
    "#   end = dt.datetime.strptime(\"2024-10-31\", \"%Y-%m-%d\")\n",
    "  \n",
    "#   soup = bs(response.text, \"html.parser\")\n",
    "#   result = soup.select(\"tbody > tr\")\n",
    "  \n",
    "#   result_list = []  \n",
    "#   try:\n",
    "#     for p in result[::-1]:\n",
    "#       state = p.get(\"class\")[0]\n",
    "#       td_list = p.select(\"td\")\n",
    "#       td = [td_list[0].getText().strip().replace(\".\", \"-\"), oil_type, state, td_list[1].getText().strip(), td_list[2].getText().strip(), td_list[3].getText().strip()[1:-1].strip()]\n",
    "      \n",
    "#       cur_date = dt.datetime.strptime(td[0], \"%Y-%m-%d\")\n",
    "#       start_diff = (cur_date - start).total_seconds()\n",
    "#       end_diff= (end - cur_date).total_seconds()\n",
    "#       if start_diff < 0 or end_diff <= 0:\n",
    "#         continue\n",
    "      \n",
    "#       result_list.append(\"('{}', '{}', '{}', {}, {}, {})\".format(td[0], td[1], td[2], td[3].replace(\",\", \"\"), td[4], td[5]))\n",
    "#   except Exception as ex:\n",
    "#     print(\"oil_bs 오류 발생\")\n",
    "#     print(ex)\n",
    "  \n",
    "#   return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 금\n",
    "- 국제금\n",
    "  - 예시 : https://finance.naver.com/marketindex/worldDailyQuote.naver?marketindexCd=CMDT_GC&fdtc=2&page=446\n",
    "- 국내금\n",
    "  - 예시 : https://finance.naver.com/marketindex/goldDailyQuote.naver?&page=305\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gold_api(gold_type):\n",
    "#   if gold_type == \"national\":\n",
    "#     HOST = \"https://finance.naver.com/marketindex/worldDailyQuote.naver?marketindexCd=CMDT_GC&fdtc=2&page=\"\n",
    "#     page = 33\n",
    "#     end = 3\n",
    "#     gold = \"국제금(달러/트로이온스)\"\n",
    "#   else :\n",
    "#     HOST = \"https://finance.naver.com/marketindex/goldDailyQuote.naver?&page=\"\n",
    "#     page = 23\n",
    "#     end = 2\n",
    "#     gold = \"국내금(원/g)\"\n",
    "  \n",
    "#   headers = {\"Content-Type\" : \"application/json\", \"charset\" : \"UTF-8\", \"Accept\":\"*/*\"}\n",
    "\n",
    "#   try:\n",
    "#     result_list = []\n",
    "#     for p in tqdm(range(page, end-1, -1), \n",
    "#               total = page-end+1, ## 전체 진행수\n",
    "#               desc = 'Desc', ## 진행률 앞쪽 출력 문장\n",
    "#               ncols = 80, ## 진행률 출력 폭 조절\n",
    "#               leave = True, ## True 반복문 완료시 진행률 출력 남김. False 남기지 않음.\n",
    "#             ):\n",
    "#       url = HOST+str(p)\n",
    "#       response = requests.get(url, headers=headers)\n",
    "#       result_list.extend(gold_bs(response, gold))\n",
    "      \n",
    "#       time.sleep(0.5)\n",
    "      \n",
    "#     return result_list\n",
    "#   except Exception as ex:\n",
    "#     print(\"gold_api 오류 발생\")\n",
    "#     print(ex)\n",
    "\n",
    "# def gold_bs(response, gold_type):\n",
    "#   start = dt.datetime.strptime(\"2024-01-01\", \"%Y-%m-%d\")\n",
    "#   end = dt.datetime.strptime(\"2024-10-31\", \"%Y-%m-%d\")\n",
    "  \n",
    "#   soup = bs(response.text, \"html.parser\")\n",
    "#   result = soup.select(\"tbody > tr\")\n",
    "  \n",
    "#   result_list = []  \n",
    "#   try:\n",
    "#     for p in result[::-1]:\n",
    "#       state = p.get(\"class\")[0]\n",
    "#       td_list = p.select(\"td\")\n",
    "      \n",
    "#       td = [td_list[0].getText().strip().replace(\".\", \"-\"), gold_type, state, td_list[1].getText().strip(), td_list[2].getText().strip()]\n",
    "#       if gold_type == \"국제금(달러/트로이온스)\" :\n",
    "#         td.append(td_list[3].getText().strip()[1:-1].strip())\n",
    "#       else :\n",
    "#         temp_value = float(td[-2].replace(\",\", \"\")) + (float(td[-1].replace(\",\", \"\")) if td[2] == 'down' else (-float(td[-1].replace(\",\", \"\"))))\n",
    "#         td.append(str(round(float(td[-1].replace(\",\", \"\")) / temp_value * 100, 2)))\n",
    "      \n",
    "#       cur_date = dt.datetime.strptime(td[0], \"%Y-%m-%d\")\n",
    "#       start_diff = (cur_date - start).total_seconds()\n",
    "#       end_diff= (end - cur_date).total_seconds()\n",
    "#       if start_diff < 0 or end_diff <= 0:\n",
    "#         continue\n",
    "      \n",
    "#       result_list.append(\"('{}', '{}', '{}', {}, {}, {})\".format(td[0], td[1], td[2], td[3].replace(\",\", \"\"), td[4].replace(\",\", \"\"), td[5]))\n",
    "#   except Exception as ex:\n",
    "#     print(\"gold_bs 오류 발생\")\n",
    "#     print(ex)\n",
    "  \n",
    "#   return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한번에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_to_pickle(file_name, data):\n",
    "#     \"\"\"Pickle 파일로 저장.\"\"\"\n",
    "#     with open(file_name, \"wb\") as f:\n",
    "#         pickle.dump(data, f)\n",
    "\n",
    "# def load_from_pickle(file_name):\n",
    "#     \"\"\"Pickle 파일에서 로드.\"\"\"\n",
    "#     with open(file_name, \"rb\") as f:\n",
    "#         return pickle.load(f)\n",
    "\n",
    "# # 데이터 처리 함수\n",
    "# def process_material_data(result_list):\n",
    "\n",
    "#     \"\"\"데이터 변환.\"\"\"\n",
    "#     processed_data = []\n",
    "#     for item in result_list:\n",
    "#         try:\n",
    "#             # 문자열을 튜플로 변환\n",
    "#             material_date, material_name, material_state, material_rate, material_change, material_change_rate = eval(item)\n",
    "\n",
    "#             # 새로운 데이터 형식으로 변환\n",
    "#             processed_data.append(\n",
    "#                 f\"('{material_date}', '{material_name}', '{material_state}', {material_rate}, {material_change}, {material_change_rate})\"\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing item: {item}, Error: {e}\")\n",
    "#             continue\n",
    "\n",
    "#     return processed_data\n",
    "\n",
    "# # MySQL 연결 및 데이터 삽입 함수\n",
    "# def insert_material_data(cur, table_name, processed_data):\n",
    "#     \"\"\"데이터 삽입.\"\"\"\n",
    "#     # 테이블 생성\n",
    "#     sql = f\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "#         material_date DATE NOT NULL,\n",
    "#         material_name VARCHAR(100) NOT NULL,\n",
    "#         material_state VARCHAR(50),\n",
    "#         material_rate FLOAT NOT NULL,\n",
    "#         material_change FLOAT,\n",
    "#         material_change_rate FLOAT\n",
    "#     );\n",
    "#     \"\"\"\n",
    "#     cur.execute(sql)\n",
    "\n",
    "#     # 데이터 삽입\n",
    "#     sql = f\"INSERT INTO {table_name} (material_date, material_name, material_state, material_rate, material_change, material_change_rate) VALUES \" + \",\".join(processed_data)\n",
    "#     cur.execute(sql)\n",
    "#     print(\"Data inserted successfully.\")\n",
    "\n",
    "# def process_and_save_data(api_function, api_param, pickle_file, table_name):\n",
    "#     \"\"\"데이터 처리 및 저장.\"\"\"\n",
    "#     # API 호출\n",
    "#     # result_list = api_function(api_param)\n",
    "\n",
    "#     # # Pickle 저장\n",
    "#     # save_to_pickle(pickle_file, result_list)\n",
    "\n",
    "#     # Pickle 로드\n",
    "#     result_list = load_from_pickle(pickle_file)\n",
    "\n",
    "#     # 데이터 변환\n",
    "#     processed_data = process_material_data(result_list)\n",
    "\n",
    "    # 데이터베이스 연결 및 삽입\n",
    "    con = pymysql.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"e3pooq^^\",\n",
    "        db=\"modoostock\",\n",
    "        charset=\"utf8mb4\",\n",
    "        autocommit=True\n",
    "    )\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        insert_material_data(cur, table_name, processed_data)\n",
    "        print(f\"Data inserted into table {table_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        con.close()\n",
    "#     # 데이터베이스 연결 및 삽입\n",
    "#     con = pymysql.connect(\n",
    "#         host=\"localhost\",\n",
    "#         user=\"ssafy\",\n",
    "#         password=\"ssafy\",\n",
    "#         db=\"modoostock\",\n",
    "#         charset=\"utf8mb4\",\n",
    "#         autocommit=True\n",
    "#     )\n",
    "#     try:\n",
    "#         cur = con.cursor()\n",
    "#         insert_material_data(cur, table_name, processed_data)\n",
    "#         print(f\"Data inserted into table {table_name}.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")\n",
    "#     finally:\n",
    "#         con.close()\n",
    "\n",
    "# # 예제 사용\n",
    "# process_and_save_data(oil_api, \"OIL_GSL\", \"data(domestic_petr_241126).p\", \"material_origin\")\n",
    "# # process_and_save_data(oil_api, \"경유\", \"data(domestic_dies_241126).p\", \"material_origin\")\n",
    "# process_and_save_data(gold_api, \"national\", \"data(domestic_glob_241126).p\", \"material_origin\")\n",
    "# process_and_save_data(gold_api, \"daily\", \"data(domestic_dome_241126).p\", \"material_origin\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주식 시세!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사용 주식 머시깽이들\n",
    "  - 전기 : 삼성전자(005930), LG전자(066570)\n",
    "  - 화학 : LG화학(051910), 롯데케미칼(011170)\n",
    "  - 생명 : 셀트리온(068270), 녹십자(006280)\n",
    "  - IT : Naver(035420), SK텔레콤(017670)\n",
    "  - 엔터 : SM엔터테이먼트(041510), JYP엔터테이먼트(035900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### api 함수"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 주식 머시깽이들\n",
    "- 예시 : https://finance.naver.com/item/sise_day.naver?code={}&page="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_api(stock_type):\n",
    "  page = 23  # 시작 페이지\n",
    "  end = 2     # 끝 페이지\n",
    "  \n",
    "  if stock_type == \"035420\":\n",
    "    stock_origin = \"네이버\"\n",
    "    stock = \"G IT\"\n",
    "  elif stock_type ==\"005930\":\n",
    "    stock_origin = \"삼성전자\"\n",
    "    stock = \"A 전자\"\n",
    "  elif stock_type ==\"051910\":\n",
    "    stock_origin = \"LG화학\"\n",
    "    stock = \"B 화학\"\n",
    "  elif stock_type ==\"068270\":\n",
    "    stock_origin = \"셀트리온\"\n",
    "    stock = \"C 생명\"\n",
    "  elif stock_type ==\"041510\":\n",
    "    stock_origin = \"SM엔터\"\n",
    "    stock = \"E 엔터\"\n",
    "  elif stock_type ==\"066570\":\n",
    "    stock_origin = \"LG전자\"\n",
    "    stock = \"F 전자\"\n",
    "  elif stock_type ==\"011170\":\n",
    "    stock_origin = \"롯데케미칼\"\n",
    "    stock = \"G 화학\"\n",
    "  elif stock_type ==\"006280\":\n",
    "    stock_origin = \"녹십자\"\n",
    "    stock = \"H 생명\"\n",
    "  elif stock_type ==\"017670\":\n",
    "    stock_origin = \"SK텔레콤\"\n",
    "    stock = \"I IT\"\n",
    "  elif stock_type ==\"035900\":\n",
    "    stock_origin = \"JYP엔터\"\n",
    "    stock = \"J 엔터\"\n",
    "  elif stock_type ==\"352820\":\n",
    "    stock_origin = \"HYBE엔터\"\n",
    "    stock = \"H 엔터\"\n",
    "  \n",
    "  HOST = \"https://finance.naver.com/item/sise_day.naver?code={}&page=\".format(stock_type)\n",
    "  \n",
    "  headers = {\"Content-Type\" : \"application/json\", \"charset\" : \"UTF-8\", \"Accept\":\"*/*\", 'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "  try:\n",
    "    result_list = []\n",
    "    dividend_rate = fetch_dividend_info(stock_type)\n",
    "    for p in tqdm(range(page, end-1, -1), \n",
    "              total = page-end+1, ## 전체 진행수\n",
    "              desc = 'Desc', ## 진행률 앞쪽 출력 문장\n",
    "              ncols = 80, ## 진행률 출력 폭 조절\n",
    "              leave = True, ## True 반복문 완료시 진행률 출력 남김. False 남기지 않음.\n",
    "            ):\n",
    "      url = HOST+str(p)\n",
    "      req = Request(url, headers=headers)\n",
    "      with urlopen(req) as response:\n",
    "        if not response is None:\n",
    "          result_list.extend(stock_bs(response, stock_origin, stock, dividend_rate))\n",
    "      \n",
    "      time.sleep(0.5)\n",
    "      \n",
    "    return result_list\n",
    "  except Exception as ex:\n",
    "    print(\"stock_api 오류 발생\")\n",
    "    print(ex)\n",
    "\n",
    "def stock_bs(response, origin, after, dividend_rate):\n",
    "    start = dt.datetime.strptime(\"2024-01-01\", \"%Y-%m-%d\")\n",
    "    end = dt.datetime.strptime(\"2024-11-01\", \"%Y-%m-%d\")\n",
    "    \n",
    "    soup = bs(response, \"html.parser\")\n",
    "    result = soup.select('table > tr[onmouseover=\"mouseOver(this)\"]')\n",
    "    \n",
    "    result_list = []  \n",
    "    try:\n",
    "        for p in result[::-1]:\n",
    "            td_list = p.select(\"td\")\n",
    "            \n",
    "            # 상태 정보 (상승/하락/보합 등)\n",
    "            state = td_list[2].select_one(\"span.blind\").getText().strip()\n",
    "            \n",
    "            # 수치 정보\n",
    "            change_value = td_list[2].select(\"span\")[-1].getText().strip().replace(\",\", \"\")\n",
    "            \n",
    "            # 배당률 사용\n",
    "            try:\n",
    "                dividend_rate = float(dividend_rate.replace('%', '').strip()) / 100 if isinstance(dividend_rate, str) else dividend_rate\n",
    "            except ValueError:\n",
    "                dividend_rate = 0  # 변환 오류가 발생하면 0으로 설정\n",
    "            \n",
    "            dividend = 0\n",
    "            try:\n",
    "                # 종가에 배당률을 곱하여 배당금 계산\n",
    "                closing_price = int(td_list[1].getText().strip().replace(\",\", \"\"))\n",
    "                dividend = int(round(closing_price * dividend_rate, 0))\n",
    "            except ValueError:\n",
    "                dividend = 0  # 만약 종가 값이 잘못된 경우 배당금은 0으로 설정\n",
    "            \n",
    "            td = [\n",
    "                td_list[0].getText().strip().replace(\".\", \"-\"),  # 날짜\n",
    "                after,                                          # 종목명\n",
    "                origin,                                         # 종목 원본\n",
    "                \"same\" if state == \"보합\" else (\"up\" if state == \"상승\" else \"down\"),           # 상태\n",
    "                td_list[1].getText().strip().replace(\",\", \"\"), # 종가\n",
    "                # state,                                          # 상태 정보 (상승/하락)\n",
    "                change_value,                                   # 수치 정보 (변화량)\n",
    "                td_list[4].getText().strip().replace(\",\", \"\"), # 시가\n",
    "                td_list[5].getText().strip().replace(\",\", \"\"), # 고가\n",
    "                td_list[6].getText().strip().replace(\",\", \"\"), # 저가\n",
    "                dividend\n",
    "            ]\n",
    "            \n",
    "            if state != \"same\":\n",
    "                change_rate = round(int(change_value) / (int(td[4]) + (int(change_value) if state == \"하락\" else -int(change_value))) * 100, 1)\n",
    "                td.append(change_rate)\n",
    "            else:\n",
    "                td.append(0)\n",
    "            \n",
    "            cur_date = dt.datetime.strptime(td[0], \"%Y-%m-%d\")\n",
    "            start_diff = (cur_date - start).total_seconds()\n",
    "            end_diff = (end - cur_date).total_seconds()\n",
    "            if start_diff < 0 or end_diff <= 0:\n",
    "                continue\n",
    "            \n",
    "            result_list.append(\"('{}', '{}', '{}', '{}', {}, '{}', {}, {}, {}, {}, {})\".format(td[0], td[1], td[2], td[3], td[4], td[5], td[6], td[7], td[8], td[9], td[10]))\n",
    "    except Exception as ex:\n",
    "        print(\"stock_bs 오류 발생\", td_list[0].getText().strip())\n",
    "        print(ex)\n",
    "    \n",
    "    return result_list\n",
    "  \n",
    "def fetch_dividend_info(stock_code):\n",
    "  url = f\"https://finance.naver.com/item/main.naver?code={stock_code}\"\n",
    "  headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "  \n",
    "  try:\n",
    "      response = requests.get(url, headers=headers)\n",
    "      response.raise_for_status()\n",
    "      soup = bs(response.text, \"html.parser\")\n",
    "      \n",
    "      dividend_rate_element = soup.select_one(\"#_dvr\")\n",
    "      if dividend_rate_element:\n",
    "          dividend_rate = dividend_rate_element.get_text(strip=True)\n",
    "          return dividend_rate\n",
    "      else:\n",
    "          print(\"No dividend rate found.\")\n",
    "          return 0\n",
    "  except Exception as ex:\n",
    "      print(f\"fetch_dividend_info 오류 발생: {ex}\")\n",
    "      return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한번에 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dividend info for stock code: 035420\n",
      "Dividend rate fetched: 0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Desc: 100%|█████████████████████████████████████| 22/22 [00:12<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dividend info for stock code: 005930\n",
      "Dividend rate fetched: 2.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Desc: 100%|█████████████████████████████████████| 22/22 [00:12<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dividend info for stock code: 051910\n",
      "Dividend rate fetched: 1.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Desc: 100%|█████████████████████████████████████| 22/22 [00:12<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dividend info for stock code: 068270\n",
      "Dividend rate fetched: 0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Desc: 100%|█████████████████████████████████████| 22/22 [00:12<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dividend info for stock code: 041510\n",
      "Dividend rate fetched: 1.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Desc: 100%|█████████████████████████████████████| 22/22 [00:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dividend info for stock code: 066570\n",
      "Dividend rate fetched: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Desc: 100%|█████████████████████████████████████| 22/22 [00:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dividend info for stock code: 011170\n",
      "Dividend rate fetched: 5.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Desc: 100%|█████████████████████████████████████| 22/22 [00:12<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dividend info for stock code: 006280\n",
      "Dividend rate fetched: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Desc: 100%|█████████████████████████████████████| 22/22 [00:12<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dividend info for stock code: 017670\n",
      "Dividend rate fetched: 6.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Desc: 100%|█████████████████████████████████████| 22/22 [00:12<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dividend info for stock code: 035900\n",
      "Dividend rate fetched: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Desc: 100%|█████████████████████████████████████| 22/22 [00:13<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# result_list = stock_api(\"035420\")\n",
    "# with open(\"data(stock_네이버_20241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(stock_네이버_20241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "result_list = stock_api(\"035420\")\n",
    "with open(\"data(stock_네이버_20241126).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)\n",
    "# with open(\"data(stock_네이버_20241126).p\", \"rb\") as f:\n",
    "#   result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^',  \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"\"\"CREATE TABLE IF NOT EXISTS stock_crawl (\n",
    "    stock_date DATE NOT NULL,               -- 주식 날짜\n",
    "    stock_name VARCHAR(50) NOT NULL,       -- 종목 이름\n",
    "    stock_name_origin VARCHAR(50) NOT NULL, -- 종목 원래 이름\n",
    "    stock_state VARCHAR(10) NOT NULL,      -- 상태 (상승/하락/보합 등)\n",
    "    stock_rate INT NOT NULL,               -- 종가\n",
    "    stock_change VARCHAR(10) NOT NULL,     -- 상태 상세 (상승/하락)\n",
    "    stock_low INT NOT NULL,                -- 저가\n",
    "    stock_high INT NOT NULL,               -- 고가\n",
    "    stock_volume INT NOT NULL,             -- 거래량\n",
    "    stock_dividend INT NOT NULL,           -- 배당금\n",
    "    stock_change_rate FLOAT NOT NULL       -- 변동률\n",
    ") CHARSET=utf8mb4;\"\"\"\n",
    "cur.execute(sql)\n",
    "sql = \"INSERT INTO stock_crawl(stock_date, stock_name, stock_name_origin, stock_state, stock_rate, stock_change, stock_high, stock_low, stock_volume, stock_dividend, stock_change_rate) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()\n",
    "\n",
    "# result_list = stock_api(\"005930\")\n",
    "# with open(\"data(stock_삼성전자_20241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(stock_삼성전자_20241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "result_list = stock_api(\"005930\")\n",
    "with open(\"data(stock_삼성전자_20241126).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)\n",
    "# with open(\"data(stock_삼성전자_20241126).p\", \"rb\") as f:\n",
    "#   result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^',  \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO stock_crawl(stock_date, stock_name, stock_name_origin, stock_state, stock_rate, stock_change, stock_high, stock_low, stock_volume, stock_dividend, stock_change_rate) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()\n",
    "\n",
    "# result_list = stock_api(\"051910\")\n",
    "# with open(\"data(stock_LG화학_20241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(stock_LG화학_20241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "result_list = stock_api(\"051910\")\n",
    "with open(\"data(stock_LG화학_20241126).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)\n",
    "# with open(\"data(stock_LG화학_20241126).p\", \"rb\") as f:\n",
    "#   result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^',  \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO stock_crawl(stock_date, stock_name, stock_name_origin, stock_state, stock_rate, stock_change, stock_high, stock_low, stock_volume, stock_dividend, stock_change_rate) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()\n",
    "\n",
    "# result_list = stock_api(\"068270\")\n",
    "# with open(\"data(stock_셀트리온_20241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(stock_셀트리온_20241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "result_list = stock_api(\"068270\")\n",
    "with open(\"data(stock_셀트리온_20241126).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)\n",
    "# with open(\"data(stock_셀트리온_20241126).p\", \"rb\") as f:\n",
    "#   result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^',  \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO stock_crawl(stock_date, stock_name, stock_name_origin, stock_state, stock_rate, stock_change, stock_high, stock_low, stock_volume, stock_dividend, stock_change_rate) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()\n",
    "\n",
    "# result_list = stock_api(\"041510\")\n",
    "# with open(\"data(stock_SM엔터_20241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(stock_SM엔터_20241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "result_list = stock_api(\"041510\")\n",
    "with open(\"data(stock_SM엔터_20241126).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)\n",
    "# with open(\"data(stock_SM엔터_20241126).p\", \"rb\") as f:\n",
    "#   result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^',  \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO stock_crawl(stock_date, stock_name, stock_name_origin, stock_state, stock_rate, stock_change, stock_high, stock_low, stock_volume, stock_dividend, stock_change_rate) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()\n",
    "\n",
    "# result_list = stock_api(\"066570\")\n",
    "# with open(\"data(stock_LG전자_20241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(stock_LG전자_20241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "result_list = stock_api(\"066570\")\n",
    "with open(\"data(stock_LG전자_20241126).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)\n",
    "# with open(\"data(stock_LG전자_20241126).p\", \"rb\") as f:\n",
    "#   result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^',   \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO stock_crawl(stock_date, stock_name, stock_name_origin, stock_state, stock_rate, stock_change, stock_high, stock_low, stock_volume, stock_dividend, stock_change_rate) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()\n",
    "\n",
    "# result_list = stock_api(\"011170\")\n",
    "# with open(\"data(stock_롯데케미칼_20241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(stock_롯데케미칼_20241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "result_list = stock_api(\"011170\")\n",
    "with open(\"data(stock_롯데케미칼_20241126).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)\n",
    "# with open(\"data(stock_롯데케미칼_20241126).p\", \"rb\") as f:\n",
    "#   result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^',   \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO stock_crawl(stock_date, stock_name, stock_name_origin, stock_state, stock_rate, stock_change, stock_high, stock_low, stock_volume, stock_dividend, stock_change_rate) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()\n",
    "\n",
    "# result_list = stock_api(\"006280\")\n",
    "# with open(\"data(stock_녹십자_20241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(stock_녹십자_20241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "result_list = stock_api(\"006280\")\n",
    "with open(\"data(stock_녹십자_20241126).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)\n",
    "# with open(\"data(stock_녹십자_20241126).p\", \"rb\") as f:\n",
    "#   result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^',  \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO stock_crawl(stock_date, stock_name, stock_name_origin, stock_state, stock_rate, stock_change, stock_high, stock_low, stock_volume, stock_dividend, stock_change_rate) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()\n",
    "\n",
    "# result_list = stock_api(\"017670\")\n",
    "# with open(\"data(stock_SK텔레콤_20241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(stock_SK텔레콤_20241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "result_list = stock_api(\"017670\")\n",
    "with open(\"data(stock_SK텔레콤_20241126).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)\n",
    "# with open(\"data(stock_SK텔레콤_20241126).p\", \"rb\") as f:\n",
    "#   result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^',  \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO stock_crawl(stock_date, stock_name, stock_name_origin, stock_state, stock_rate, stock_change, stock_high, stock_low, stock_volume, stock_dividend, stock_change_rate) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()\n",
    "\n",
    "# result_list = stock_api(\"035900\")\n",
    "# with open(\"data(stock_JYP엔터_20241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(stock_JYP엔터_20241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "result_list = stock_api(\"035900\")\n",
    "with open(\"data(stock_JYP엔터_20241126).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)\n",
    "# with open(\"data(stock_JYP엔터_20241126).p\", \"rb\") as f:\n",
    "#   result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^',  \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO stock_crawl(stock_date, stock_name, stock_name_origin, stock_state, stock_rate, stock_change, stock_high, stock_low, stock_volume, stock_dividend, stock_change_rate) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dividend info for stock code: 352820\n",
      "Dividend rate fetched: 0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Desc: 100%|█████████████████████████████████████| 22/22 [00:12<00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "result_list = stock_api(\"352820\")\n",
    "with open(\"data(stock_HYBE엔터_20241126).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)\n",
    "# with open(\"data(stock_HYBE엔터_20241126).p\", \"rb\") as f:\n",
    "  # result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "result_list = stock_api(\"352820\")\n",
    "with open(\"data(stock_HYBE엔터_20241126).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)\n",
    "# with open(\"data(stock_HYBE엔터_20241126).p\", \"rb\") as f:\n",
    "  # result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^',  \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO stock_crawl(stock_date, stock_name, stock_name_origin, stock_state, stock_rate, stock_change, stock_high, stock_low, stock_volume, stock_dividend, stock_change_rate) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주식 뉴스!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사용 주식 머시깽이들\n",
    "  - 전기 : 삼성전자(005930), LG전자(066570)\n",
    "  - 화학 : LG화학(051910), 롯데케미칼(011170)\n",
    "  - 생명 : 셀트리온(068270), 녹십자(006280)\n",
    "  - IT : Naver(035420), SK텔레콤(017670)\n",
    "  - 엔터 : SM엔터테이먼트(041510), JYP엔터테이먼트(035900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### api 함수"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예시 : https://search.naver.com/search.naver?where=news&query={키워드}&sm=tab_opt&sort=0&photo=3&field=0&pd=3&ds={YYYY.MM.DD}&de={YYYY.MM.DD}&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom{YYYYMMDD}to{YYYYMMDD}&is_sug_officeid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def news_api(news_type):\n",
    "    if news_type == \"네이버\":\n",
    "        after = \"G IT\"\n",
    "    elif news_type == \"삼성전자\":\n",
    "        after = \"A 전자\"\n",
    "    elif news_type == \"LG화학\":\n",
    "        after = \"B 화학\"\n",
    "    elif news_type == \"셀트리온\":\n",
    "        after = \"C 생명\"\n",
    "    elif news_type == \"SM엔터\":\n",
    "        after = \"E 엔터\"\n",
    "    elif news_type == \"LG전자\":\n",
    "        after = \"F 전자\"\n",
    "    elif news_type == \"롯데케미칼\":\n",
    "        after = \"G 화학\"\n",
    "    elif news_type == \"녹십자\":\n",
    "        after = \"H 생명\"\n",
    "    elif news_type == \"SK텔레콤\":\n",
    "        after = \"I IT\"\n",
    "    elif news_type == \"JYP엔터\":\n",
    "        after = \"J 엔터\"\n",
    "    elif news_type == \"하이브\":\n",
    "        after = \"H 엔터\"\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\", \"charset\": \"UTF-8\", \"Accept\": \"*/*\", 'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    result_list = []\n",
    "    result_list_all = []\n",
    "    st = dt.datetime.now()\n",
    "    print(\"시작 시간 : {}\".format(st))\n",
    "    keyword = urllib.parse.quote(news_type)\n",
    "    \n",
    "    for cur_date in pd.date_range(start=\"1/1/2024\", end=\"10/31/2024\"):\n",
    "        cur = cur_date.strftime(\"%Y.%m.%d\")\n",
    "        \n",
    "        try:\n",
    "            url = \"https://search.naver.com/search.naver?where=news&query={keyword}&sm=tab_opt&sort=0&photo=3&field=0&pd=3&ds={date1}&de={date1}&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom{date2}to{date2}&is_sug_officeid=0\".format(keyword=keyword, date1=cur, date2=cur.replace(\".\", \"\"))\n",
    "            req = Request(url, headers=headers)\n",
    "            with urlopen(req) as response:\n",
    "                if response is not None:\n",
    "                    # 뉴스에서 딱 하나만 가져오도록 변경\n",
    "                    new_result_list, new_result_list_all = news_bs(response, news_type, after)\n",
    "                    if new_result_list:  # 결과가 있으면 추가\n",
    "                        result_list.append(new_result_list[0])  # 날짜별 첫 번째 뉴스만 저장\n",
    "                        result_list_all.extend(new_result_list_all)\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(\"news_api 오류 발생\")\n",
    "            print(ex)\n",
    "        finally:\n",
    "            time.sleep(0.3)\n",
    "    \n",
    "    et = dt.datetime.now()\n",
    "    print(\"종료 시간 : {}\".format(et))\n",
    "    diff = int((et-st).total_seconds())\n",
    "    total_hour = int(diff // 3600)\n",
    "    diff = int(diff % 3600)\n",
    "    total_minute = int(diff // 60)\n",
    "    total_second = int(diff % 60)\n",
    "    print(\"소요 시간 : {}시간 {}분 {}초\".format(total_hour, total_minute, total_second))\n",
    "    \n",
    "    return result_list, result_list_all\n",
    "\n",
    "\n",
    "def news_bs(response, origin, after):\n",
    "    soup = bs(response, \"lxml\")\n",
    "    result = soup.select(\"ul.list_news > li.bx\")  # 기사 타이틀이 들어있는 태그 리스트\n",
    "    \n",
    "    result_list = []  # 결과 저장(기업명 포함 기사만 저장)\n",
    "    result_list_all = []  # 모든 결과 저장\n",
    "    try:\n",
    "        if result:  # 결과가 있으면 첫 번째 뉴스만 처리\n",
    "            p = result[0]  # 첫 번째 뉴스만 선택\n",
    "            try:\n",
    "                # 날짜 필터링\n",
    "                cur_date_raw = p.select(\".info\")[2].get_text().strip()  # 원본 날짜 값\n",
    "                cur_date = cur_date_raw[:-1].replace(\".\", \"-\")  # 포맷 변경\n",
    "                datetime.strptime(cur_date, \"%Y-%m-%d\")  # 유효하지 않으면 예외 발생\n",
    "                \n",
    "                # 타이틀 추출\n",
    "                cur_title = p.select_one(\".news_tit\").get_attribute_list(\"title\")[0]\n",
    "                cur_title = cur_title.replace(\"`\", '\"').replace(\"'\", '\"')\n",
    "                \n",
    "                # 결과 추가\n",
    "                result = \"('{}', '{}', '{}', '{}')\".format(cur_date, origin, after, cur_title)\n",
    "                result_list_all.append(result)\n",
    "                if origin in cur_title:\n",
    "                    result_list.append(result)\n",
    "            except ValueError:\n",
    "                # 날짜 형식이 올바르지 않으면 건너뜀\n",
    "                pass\n",
    "    except Exception as ex:\n",
    "        print(\"news_bs 오류 발생\")\n",
    "        print(ex)\n",
    "    \n",
    "    return result_list, result_list_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시작 시간 : 2024-12-07 04:02:02.435219\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n",
      "news_api 오류 발생\n",
      "HTTP Error 403: Forbidden\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_list, result_list_all \u001b[38;5;241m=\u001b[39m \u001b[43mnews_api\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m네이버\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata(news_네이버_241203).p\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m   pickle\u001b[38;5;241m.\u001b[39mdump(result_list, f)\n",
      "Cell \u001b[0;32mIn[26], line 53\u001b[0m, in \u001b[0;36mnews_api\u001b[0;34m(news_type)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28mprint\u001b[39m(ex)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m et \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m종료 시간 : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(et))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result_list, result_list_all = news_api(\"네이버\")\n",
    "with open(\"data(news_네이버_241203).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list, result_list_all = news_api(\"삼성전자\")\n",
    "with open(\"data(news_삼성전자_241203).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list, result_list_all = news_api(\"셀트리온\")\n",
    "with open(\"data(news_셀트리온_241203).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list, result_list_all = news_api(\"하이브\")\n",
    "with open(\"data(news_HYBE엔터_241203).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list, result_list_all = news_api(\"LG화학\")\n",
    "with open(\"data(news_LG화학_241203).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list, result_list_all = news_api(\"SM엔터\")\n",
    "with open(\"data(news_SM엔터_241203).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list, result_list_all = news_api(\"LG전자\")\n",
    "with open(\"data(news_LG전자_241203).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list, result_list_all = news_api(\"롯데케미칼\")\n",
    "with open(\"data(news_롯데케미칼_241203).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list, result_list_all = news_api(\"녹십자\")\n",
    "with open(\"data(news_녹십자_241203).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list, result_list_all = news_api(\"SK텔레콤\")\n",
    "with open(\"data(news_SK텔레콤_241203).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한번에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_list, result_list_all = news_api(\"네이버\")\n",
    "# with open(\"data(news_네이버_241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(news_네이버_241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"\"\"CREATE TABLE news_origin (\n",
    "    news_date DATE NOT NULL,               -- 뉴스 날짜\n",
    "    news_name_origin VARCHAR(50) NOT NULL, -- 원본 뉴스 이름\n",
    "    news_name VARCHAR(50) NOT NULL,       -- 변환된 뉴스 이름\n",
    "    news_content TEXT NOT NULL,           -- 뉴스 내용 (기사 제목)\n",
    "    PRIMARY KEY (news_date, news_name_origin, news_content(255)) -- 고유 키 설정 (옵션)\n",
    ") CHARSET=utf8mb4;\"\"\"\n",
    "cur.execute(sql)\n",
    "sql = \"INSERT INTO news_origin(news_date, news_name_origin, news_name, news_content) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()\n",
    "\n",
    "# result_list, result_list_all = news_api(\"삼성전자\")\n",
    "# with open(\"data(news_삼성전자_241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(news_삼성전자_241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO news_origin(news_date, news_name_origin, news_name, news_content) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()\n",
    "\n",
    "# result_list, result_list_all = news_api(\"LG화학\")\n",
    "# with open(\"data(news_LG화학_241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(news_LG화학_241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO news_origin(news_date, news_name_origin, news_name, news_content) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()\n",
    "\n",
    "# result_list, result_list_all = news_api(\"셀트리온\")\n",
    "# with open(\"data(news_셀트리온_241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(news_셀트리온_241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO news_origin(news_date, news_name_origin, news_name, news_content) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()\n",
    "\n",
    "# result_list, result_list_all = news_api(\"SM엔터\")\n",
    "# with open(\"data(news_SM엔터_241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(news_SM엔터_241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO news_origin(news_date, news_name_origin, news_name, news_content) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()\n",
    "\n",
    "# result_list, result_list_all = news_api(\"LG전자\")\n",
    "# with open(\"data(news_LG전자_241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(news_LG전자_241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^',  \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO news_origin(news_date, news_name_origin, news_name, news_content) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_list, result_list_all = news_api(\"롯데케미칼\")\n",
    "# with open(\"data(news_롯데케미칼_241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "# with open(\"data(news_롯데케미칼_241126).p\", \"rb\") as f:\n",
    "#   result_list = pickle.load(f)\n",
    "# con = pymysql.connect(host='localhost', user='ssafy', password='ssafy', \n",
    "#                       db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "# cur = con.cursor()\n",
    "# sql = \"INSERT INTO news_origin(news_date, news_name_origin, news_name, news_content) VALUES \" + \",\".join(result_list)\n",
    "# cur.execute(sql)\n",
    "# con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_list, result_list_all = news_api(\"녹십자\")\n",
    "# with open(\"data(news_녹십자_241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(news_녹십자_241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^',  \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO news_origin(news_date, news_name_origin, news_name, news_content) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_list, result_list_all = news_api(\"SK텔레콤\")\n",
    "# with open(\"data(news_SK텔레콤_241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "with open(\"data(news_SK텔레콤_241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO news_origin(news_date, news_name_origin, news_name, news_content) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_list, result_list_all = news_api(\"JYP엔터\")\n",
    "# with open(\"data(news_JYP엔터_241126).p\", \"wb\") as f:\n",
    "#   pickle.dump(result_list, f)\n",
    "# with open(\"data(news_JYP엔터_241126).p\", \"rb\") as f:\n",
    "#   result_list = pickle.load(f)\n",
    "# con = pymysql.connect(host='localhost', user='ssafy', password='ssafy', \n",
    "#                       db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "# cur = con.cursor()\n",
    "# sql = \"INSERT INTO news_origin(news_date, news_name_origin, news_name, news_content) VALUES \" + \",\".join(result_list)\n",
    "# cur.execute(sql)\n",
    "# con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list, result_list_all = news_api(\"하이브\")\n",
    "with open(\"data(news_HYBE엔터_241126).p\", \"wb\") as f:\n",
    "  pickle.dump(result_list, f)\n",
    "with open(\"data(news_HYBE엔터_241126).p\", \"rb\") as f:\n",
    "  result_list = pickle.load(f)\n",
    "con = pymysql.connect(host='localhost', user='root', password='e3pooq^^', \n",
    "                      db='modoostock', charset='utf8mb4', autocommit=True)\n",
    "cur = con.cursor()\n",
    "sql = \"INSERT INTO news_origin(news_date, news_name_origin, news_name, news_content) VALUES \" + \",\".join(result_list)\n",
    "cur.execute(sql)\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
